{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import pandas as pd\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Reviews as DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('reviews.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init functions to remove and extract hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeHashTags(review_str):\n",
    "    \n",
    "    '''\n",
    "    Input: Review in string format.\n",
    "    Output: String of the review with starting hashtags all removed.\n",
    "            (hashtags in the middle of reviews will not be removed\n",
    "            to preserve phrasing)\n",
    "    \n",
    "    E.g.\n",
    "    \n",
    "    removeHashTags(\"#Amazing #Delicious #Foodcoma #iHateHashTags The wagyu beef was the #BEST i've every tried\")\n",
    "    \n",
    "    >> The wagyu beef was the BEST i've ever tried.\n",
    "    '''\n",
    "    \n",
    "    review = review_str.split()\n",
    "    \n",
    "    temp = 0\n",
    "    \n",
    "    for i in range(len(review)):\n",
    "        \n",
    "        text = review[i]\n",
    "        \n",
    "        if text[0] == '#' and temp == i:\n",
    "            review[i] = ' '\n",
    "            temp += 1\n",
    "            \n",
    "        elif text[0] == '#':\n",
    "            review[i] = review[i].lstrip('#')\n",
    "    \n",
    "    return ' '.join(review).lstrip()\n",
    "\n",
    "\n",
    "###-------------------------------------------###\n",
    "###-------------------------------------------###\n",
    "###-------------------------------------------###\n",
    "\n",
    "\n",
    "def getHashTags(review_str):\n",
    "    \n",
    "    '''\n",
    "    Input: review text in string format.\n",
    "    Output: List of HashTags\n",
    "    \n",
    "    e.g. getHashTags('The #amazing wagyu #beef was so #juicy')\n",
    "    >>> [#amazing, #beef, #juicy]\n",
    "    '''\n",
    "    \n",
    "    return re.findall('#\\w+',review_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A 2-stage process is used to extract all nouns and adjective phrases.\n",
    "\n",
    "<ol>\n",
    "    <li>We __extract all unigram nouns__ and append it into the \"results\" list.</li>\n",
    "<li>Next, we __extract bigram and trigram units__ that have predefined POS tag sequences, and continue appending the extracted results to the \"results\" list. The current implementation below uses 7 rules.</li>\n",
    " </ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store all results in this list\n",
    "results = []\n",
    "\n",
    "for i in range(data.shape[0]):\n",
    "    rev = data['Unnamed: 0'].iloc[i]\n",
    "    rev = rev.replace('#','')\n",
    "    doc = nlp(rev)\n",
    "\n",
    "    for w in doc:\n",
    "        if w.pos_ == 'NOUN':\n",
    "            results.append(str(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS and Dependency Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = []\n",
    "\n",
    "# For each review\n",
    "for n in range(data.shape[0]):\n",
    "    \n",
    "    rev = data['Review'].iloc[n]\n",
    "        \n",
    "    try:\n",
    "        rev = rev.replace('\\n', ' ')\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    rev = removeHashTags(rev)\n",
    "    doc = nlp(rev)\n",
    "\n",
    "    text_list = []\n",
    "    pos_list =[]\n",
    "    dep_list=[]\n",
    "    \n",
    "    # For each word in the review\n",
    "    for w in doc:\n",
    "        \n",
    "        extract = None\n",
    "        \n",
    "        text_list.append(str(w.text))\n",
    "        pos_list.append(w.pos_)\n",
    "        dep_list.append(w.dep_)\n",
    "        \n",
    "        if len(text_list) > 3:\n",
    "            # Rule 1\n",
    "            if pos_list[-1] in ['NOUN', 'PROPN'] and pos_list[-2] in ['NOUN', 'PROPN'] and pos_list[-3] == 'ADJ':\n",
    "                extract = text_list[-3] + ' ' +  text_list[-2] + ' ' + text_list[-1]\n",
    "\n",
    "            # Rule 2\n",
    "            elif pos_list[-1] in ['NOUN', 'PROPN'] and pos_list[-2] in ['NOUN', 'PROPN'] and pos_list[-3] == 'VERB':\n",
    "                extract = text_list[-3] + ' ' + text_list[-2] + ' ' + text_list[-1]\n",
    "\n",
    "            # Rule 3\n",
    "            elif pos_list[-1] in ['NOUN', 'PROPN'] and pos_list[-2] in ['NOUN', 'PROPN'] and pos_list[-3] == 'NOUN':\n",
    "                extract = text_list[-3] + ' ' + text_list[-2] + ' ' + text_list[-1]\n",
    "\n",
    "            # Rule 4\n",
    "            elif pos_list[-1] in ['NOUN', 'PROPN'] and pos_list[-2] in ['NOUN', 'PROPN']:\n",
    "                extract = text_list[-2] + ' ' + text_list[-1]\n",
    "\n",
    "            # Rule 5\n",
    "            elif pos_list[-1] in ['NOUN', 'PROPN'] and pos_list[-2] == 'VERB':\n",
    "                extract = text_list[-2] + ' ' + text_list[-1]\n",
    "\n",
    "            # Rule 6\n",
    "            elif pos_list[-1] in ['NOUN', 'PROPN'] and pos_list[-2] == 'ADJ':\n",
    "                extract = text_list[-2] + ' ' + text_list[-1]\n",
    "            \n",
    "            # Rule 7\n",
    "            if pos_list[-1] in ['NOUN', 'PROPN'] and dep_list[-1]=='dobj':\n",
    "                extract = text_list[-2] + ' ' + text_list[-1]\n",
    "         \n",
    "            if extract != None:\n",
    "         \n",
    "                results.append(extract)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format and export results as txt file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns all unique elements in the \"results\" list, and sorts all elements from A-Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Export successful: 9631 phrases\n"
     ]
    }
   ],
   "source": [
    "unique_results = sorted(list(set(results)))\n",
    "\n",
    "# Export\n",
    "with open('master_wordlist.txt', 'w', encoding='utf-8') as f:\n",
    "    for text in unique_results:\n",
    "        f.write(text +'\\n')\n",
    "\n",
    "print(\"Export successful: {} phrases\".format(len(unique_results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
