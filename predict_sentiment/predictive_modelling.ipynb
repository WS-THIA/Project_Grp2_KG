{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train sentiment prediction model based on amazon food reviews\n",
    "This dataset consists of reviews of fine foods from amazon. The data span a period of more than 10 years, including all ~500,000 reviews up to October 2012. Reviews include product and user information, ratings, and a plain text review. It also includes reviews from all other Amazon categories.\n",
    "\n",
    "https://www.kaggle.com/snap/amazon-fine-food-reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram + Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9487177048962088\n",
      "\n",
      "Top 20 positive sentiments\n",
      "             Word  Coefficient\n",
      "            great    21.086667\n",
      "             best    17.959149\n",
      "        delicious    17.431698\n",
      "          perfect    14.991942\n",
      "            loves    13.772703\n",
      "             love    13.432053\n",
      "        excellent    13.406393\n",
      "        wonderful    11.630547\n",
      "             good    11.433786\n",
      "             nice    10.750654\n",
      "         favorite    10.629300\n",
      "          amazing     9.941800\n",
      "          awesome     9.549840\n",
      "          pleased     9.233244\n",
      "             easy     9.184555\n",
      "            happy     8.988801\n",
      "           smooth     8.802296\n",
      "            yummy     8.801215\n",
      "            tasty     8.652572\n",
      " highly recommend     8.611649\n",
      "\n",
      "Top 20 negative sentiments\n",
      "           Word  Coefficient\n",
      "          maybe    -7.921479\n",
      "         hoping    -8.101693\n",
      "          money    -8.208116\n",
      "      tasteless    -8.289617\n",
      "        thought    -8.531554\n",
      "          worse    -8.570159\n",
      "     disgusting    -9.004960\n",
      "          bland    -9.366393\n",
      "          threw    -9.630638\n",
      "          stale   -10.022311\n",
      "           weak   -10.078965\n",
      "         return   -10.342266\n",
      " disappointment   -10.590820\n",
      "  unfortunately   -11.087127\n",
      "       horrible   -11.473599\n",
      "          awful   -12.050633\n",
      "       terrible   -13.381042\n",
      "  disappointing   -13.739227\n",
      "   disappointed   -14.654536\n",
      "          worst   -15.576714\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "#=====================================================#\n",
    "# Create train and test dataset from kaggle data\n",
    "#=====================================================#\n",
    "df = pd.read_csv('Amazon_food.csv') #read csv\n",
    "amazon = df[df['Score'] != 3] #remove neutral score/ feedback rating (i.e. =3)\n",
    "X = amazon['Text'] #extract reviews\n",
    "y = amazon['Score'].map({1:0, 2:0, 4:1, 5:1}) #extract score\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size =0.2, random_state=0) # 80/20 split\n",
    "\n",
    "\n",
    "#=====================================================#\n",
    "# Train the model\n",
    "#=====================================================#\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), stop_words = 'english') #for converting reviews to matrix of TF-IDF features\n",
    "lr = LogisticRegression(random_state=0, solver='lbfgs') #using logistic regression as prediction model\n",
    "clf_pipeline = Pipeline([('tfidf', tfidf), ('lr', lr)]) #create pipeline of vectorizing and prediction steps\n",
    "clf_pipeline.fit(X_train, y_train) #fit training data into pipeline\n",
    "print ('Accuracy: {}'.format(clf_pipeline.score(X_test, y_test))) #get accuracy\n",
    "\n",
    "\n",
    "#=====================================================#\n",
    "# Save the model to disk\n",
    "#=====================================================#\n",
    "filename = 'lr_model.sav'\n",
    "joblib.dump(clf_pipeline, filename) #persist model into file \n",
    "\n",
    "\n",
    "#=====================================================#\n",
    "# Extract postive and negative sentiments\n",
    "#=====================================================#\n",
    "num = 20 #number of features\n",
    "\n",
    "word = tfidf.get_feature_names() #get sentiment words/ features\n",
    "coef = clf_pipeline.named_steps['lr'].coef_.tolist()[0] #get coefficient of features (feature importances) \n",
    "coeff_df = pd.DataFrame({'Word' : word, 'Coefficient' : coef})\n",
    "coeff_df = coeff_df.sort_values(['Coefficient'], ascending=[False]) #sort descending by coefficient.\n",
    "\n",
    "print('\\n'+'Top {} positive sentiments'.format(num))\n",
    "print(coeff_df.head(num).to_string(index=False))\n",
    "print('\\n'+'Top {} negative sentiments'.format(num))   \n",
    "print(coeff_df.tail(num).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Accuracy: 0.935205347888516\n",
    "\n",
    "Top 20 positive sentiments\n",
    "       Word  Coefficient\n",
    "      great    14.056236\n",
    "  delicious    12.569577\n",
    "       best    11.917344\n",
    "    perfect    10.765441\n",
    "  excellent     9.978509\n",
    "      loves     9.678879\n",
    "     highly     8.902305\n",
    "       love     8.513440\n",
    "  wonderful     8.197428\n",
    "    amazing     7.540611\n",
    " pleasantly     7.471089\n",
    "       good     7.316470\n",
    "     hooked     7.281038\n",
    "    awesome     7.221646\n",
    "       nice     7.099952\n",
    "   favorite     6.921111\n",
    "    pleased     6.900758\n",
    "      yummy     6.697903\n",
    "     smooth     6.697484\n",
    "       glad     6.421410\n",
    "\n",
    "Top 20 negative sentiments\n",
    "           Word  Coefficient\n",
    "          waste    -5.656552\n",
    "           yuck    -5.713675\n",
    "   unacceptable    -5.917392\n",
    "    undrinkable    -5.942775\n",
    "          worse    -6.134979\n",
    "          stale    -6.234432\n",
    "     disgusting    -6.409609\n",
    "      tasteless    -6.449217\n",
    "           weak    -6.489440\n",
    "          bland    -6.605334\n",
    "          threw    -6.798688\n",
    "         return    -6.892879\n",
    "  unfortunately    -7.706355\n",
    "       horrible    -7.912724\n",
    " disappointment    -8.059012\n",
    "          awful    -8.314880\n",
    "   disappointed    -8.523823\n",
    "       terrible    -9.373132\n",
    "  disappointing    -9.852226\n",
    "          worst   -11.512352\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Accuracy: 0.9206279775206109\n",
    "\n",
    "Top 20 positive sentiments\n",
    "                 Word  Coefficient\n",
    "     highly recommend    14.685871\n",
    "           just right    10.431683\n",
    "   highly recommended     9.097240\n",
    "        great product     9.026870\n",
    " pleasantly surprised     8.575248\n",
    "         tastes great     8.531111\n",
    "         great flavor     7.523886\n",
    "          really good     7.471172\n",
    "             far best     7.212022\n",
    "         best tasting     6.611659\n",
    "           love stuff     6.536895\n",
    "        great tasting     6.434235\n",
    "          taste great     6.413230\n",
    "            dogs love     6.292232\n",
    "          great price     6.265570\n",
    "          great taste     6.044865\n",
    "            dog loves     6.037769\n",
    "        thanks amazon     5.905563\n",
    "        free shipping     5.898499\n",
    "            just love     5.860198\n",
    "\n",
    "Top 20 negative sentiments\n",
    "                 Word  Coefficient\n",
    "           throw away    -7.044931\n",
    "         wasted money    -7.187337\n",
    "          tastes like    -7.212991\n",
    "            bad batch    -7.230827\n",
    "      quality control    -7.340720\n",
    "            way sweet    -7.386776\n",
    "  really disappointed    -7.561356\n",
    "          wanted like    -7.856835\n",
    "           save money    -8.110463\n",
    "           high hopes    -8.114120\n",
    "              don buy    -8.188355\n",
    "             did like    -8.414479\n",
    "             ll stick    -8.453697\n",
    " disappointed product    -8.515903\n",
    "              won buy    -8.700307\n",
    "           threw away    -9.161024\n",
    "         buyer beware    -9.472091\n",
    "          tasted like   -10.302652\n",
    "           won buying   -10.492836\n",
    "          waste money   -16.873954\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigram result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Accuracy: 0.878474368361496\n",
    "\n",
    "Top 20 positive sentiments\n",
    "                     Word  Coefficient\n",
    " highly recommend product     7.592842\n",
    "              br br great     5.794580\n",
    "             br br highly     5.404922\n",
    "            goes long way     5.238101\n",
    "           love love love     4.803285\n",
    "      br highly recommend     4.463377\n",
    "    br highly recommended     3.976873\n",
    "        amazon best price     3.901483\n",
    "         best gluten free     3.808341\n",
    "          taste just like     3.767422\n",
    "            br br overall     3.670306\n",
    "         great cup coffee     3.616616\n",
    "      great product great     3.604893\n",
    "          just right size     3.577530\n",
    "        hard time finding     3.528331\n",
    "    amazon subscribe save     3.376714\n",
    "              br br enjoy     3.290670\n",
    "               br br love     3.290012\n",
    "         tastes just like     3.232508\n",
    "          ve tried brands     3.227960\n",
    "\n",
    "Top 20 negative sentiments\n",
    "                    Word  Coefficient\n",
    "         didn taste good    -4.508626\n",
    "       wish read reviews    -4.634678\n",
    " amazon customer service    -4.674480\n",
    "             br br sorry    -4.713612\n",
    "         threw rest away    -4.797813\n",
    "      br br disappointed    -5.017063\n",
    "        waste time money    -5.347787\n",
    "         does taste like    -5.481824\n",
    "          save money buy    -5.522929\n",
    "  really looking forward    -5.586859\n",
    "        doesn taste good    -5.753869\n",
    "          did taste like    -5.819811\n",
    "    complete waste money    -5.894788\n",
    "         didn taste like    -5.903783\n",
    "     ended throwing away    -6.316694\n",
    "       total waste money    -7.423270\n",
    "        doesn taste like    -7.428759\n",
    "           got bad batch    -8.011932\n",
    "      really wanted like    -9.301142\n",
    "         don waste money   -15.864513\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
